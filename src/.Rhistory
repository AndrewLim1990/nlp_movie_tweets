sum(1,2,3)
sum(1,2,3,5,4,2,3,4,3,2,2)
library(lubridate)
library(tidyverse)
library(tidytext)
conform_weight <- 0.4
favorite_weight <- 0.1
retweet_weight <- 0.1
time_diff_now_weight <- 0.4
assert_that(sum(conform_weight, favorite_weight, retweet_weight, time_diff_now_weight) == 1)
library(assertthat)
assert_that(sum(conform_weight, favorite_weight, retweet_weight, time_diff_now_weight) == 1)
tweets <- read.csv("../data/candidate_tweets.csv")
tweets
cleaned_tweet_words <- clean_tweets(tweets)
clean_tweets <- function(tweets) {
# Coerce the tweet text from factor to character,
# so that the text can be split into words.
tweets$text <- as.character(tweets$text)
# Get one word per row.
tidy_tweet_words <- tweets %>%
unnest_tokens(word, text)
# Clean out highly common and uninformative words.
cleaned_tweet_words <- tidy_tweet_words %>%
# Remove stop words.
anti_join(stop_words) %>%
# Remove html and retweet tokens.
filter(word != "https",
word != "t.co",
word != "rt")
return(cleaned_tweet_words)
}
cleaned_tweet_words <- clean_tweets(tweets)
cleaned_tweet_words
